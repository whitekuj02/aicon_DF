{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aicontest/anaconda3/envs/moon/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "\n",
    "model = model.to('cuda:0')\n",
    "\n",
    "def inference_top_k(fpath, k=5, vname='Sliding door', with_logit=False):\n",
    "    data, sr = torchaudio.load(fpath)\n",
    "    data = torchaudio.functional.resample(data, orig_freq=sr, new_freq=16000)\n",
    "    data = data.squeeze()\n",
    "\n",
    "    # audio file is decoded on the fly\n",
    "    inputs = feature_extractor(data, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    inputs = inputs.to('cuda:0')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    logits = logits.squeeze()\n",
    "\n",
    "    predicted_class_ids = torch.argsort(logits)[-k:]\n",
    "    predicted_label = [model.config.id2label[_id.item()] for _id in predicted_class_ids]\n",
    "\n",
    "    if with_logit:\n",
    "        return predicted_label, sorted(logits)[-k:]\n",
    "    else:\n",
    "        return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [12:11<00:00, 68.39it/s]  \n"
     ]
    }
   ],
   "source": [
    "tqdm_bar = tqdm(glob('/home/aicontest/DF/data/audio/test/*.ogg'))\n",
    "for file in tqdm_bar:\n",
    "    removed = moons_removal(file)\n",
    "\n",
    "    file_name = file.split(\"/\")[-1][:-3] + \"wav\"\n",
    "    file_path = f\"/home/aicontest/DF/data/audio/moon_test_data/{file_name}\"\n",
    "    torchaudio.save(file_path, removed, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_speech_list = []\n",
    "\n",
    "with open(\"../Audio-Denoising/non_speech.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        file_path = line.split(' ')[0]\n",
    "        file_name = file_path.split('/')[-1][:-4]\n",
    "        non_speech_list.append(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fake</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.999947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>0.999965</td>\n",
       "      <td>0.987061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.097766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>TEST_49995</td>\n",
       "      <td>0.022392</td>\n",
       "      <td>0.966091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>TEST_49996</td>\n",
       "      <td>0.998367</td>\n",
       "      <td>0.008262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>TEST_49997</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.965079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>TEST_49998</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.999932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>TEST_49999</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>0.042426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id      fake      real\n",
       "0      TEST_00000  0.999667  0.999947\n",
       "1      TEST_00001  0.999965  0.987061\n",
       "2      TEST_00002  0.999998  0.097766\n",
       "3      TEST_00003  0.000035  0.999998\n",
       "4      TEST_00004  1.000000  0.000006\n",
       "...           ...       ...       ...\n",
       "49995  TEST_49995  0.022392  0.966091\n",
       "49996  TEST_49996  0.998367  0.008262\n",
       "49997  TEST_49997  0.999999  0.965079\n",
       "49998  TEST_49998  0.000326  0.999932\n",
       "49999  TEST_49999  0.999945  0.042426\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission_df = pd.read_csv(\"/home/aicontest/DF/result/uijin/submit_240703_173537.csv\")\n",
    "submission_df.loc[submission_df['id'].isin(non_speech_list), ['fake', 'real']] = 0\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"./masked_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Inside, small room', 'Music', 'Speech', 'Siren', 'Civil defense siren']\n",
      "[tensor(-4.8782, device='cuda:0'), tensor(-3.5884, device='cuda:0'), tensor(0.1119, device='cuda:0'), tensor(0.1708, device='cuda:0'), tensor(0.3438, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/aicontest/DF/data/audio/test/TEST_00178.ogg\"\n",
    "labels, prob = inference_top_k(fpath=path, k=5, with_logit=True)\n",
    "\n",
    "print(labels)\n",
    "print(prob)\n",
    "if 'Narration, monologue' in labels and 'Speech' in labels:\n",
    "    print(\"This!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/55438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2567/55438 [02:28<51:06, 17.24it/s, only_speech=defaultdict(<class 'int'>, {'Male speech, man speaking': 761, 'Speech': 2565, 'Female speech, woman speaking': 379, 'Speech synthesizer': 580, 'Narration, monologue': 311, 'Sound effect': 68, 'Inside, small room': 34, 'Clicking': 7, 'White noise': 9, 'Heart sounds, heartbeat': 13, 'Hum': 17, 'Gasp': 19, 'Animal': 31, 'Mains hum': 2, 'Spray': 3, 'Conversation': 44, 'Raindrop': 1, 'Owl': 3, 'Zipper (clothing)': 29, 'Knock': 5, 'Music': 30, 'Chopping (food)': 4, 'Bird': 2, 'Chink, clink': 12, 'Oink': 6, 'Throat clearing': 24, 'Slap, smack': 30, 'Tick-tock': 5, 'Single-lens reflex camera': 30, 'Frog': 6, 'Biting': 2, 'Static': 6, 'Vehicle': 8, 'Chop': 1, 'Mouse': 8, 'Meow': 3, 'Finger snapping': 3, 'Crunch': 4, 'Door': 1, 'Typing': 1, 'Whistle': 2, 'Television': 14, 'Rain on surface': 2, 'Coo': 1, 'Rustling leaves': 2, 'Pink noise': 2, 'Telephone': 1, 'Writing': 1, 'Plop': 2, 'Cattle, bovinae': 2, 'Tick': 5, 'Water tap, faucet': 1, 'Computer keyboard': 2, 'Cash register': 1, 'Liquid': 1, 'Beep, bleep': 2, 'Child singing': 1, 'Boiling': 3, 'Squish': 2, 'Heart murmur': 2, 'Radio': 1, 'Crow': 1, 'Fart': 1, 'Caw': 1, 'Ding': 2, 'Chewing, mastication': 1, 'Hoot': 1, 'Domestic animals, pets': 1, 'Grunt': 1, 'Slam': 1, 'Vehicle horn, car horn, honking': 1, 'Scratch': 1, 'Coin (dropping)': 1, 'Drip': 1, 'Whistling': 1, 'Snort': 1, 'Bow-wow': 1})]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./test_only_speech_list.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tf:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m tqdm_bar:\n\u001b[0;32m---> 17\u001b[0m         labels, prob \u001b[38;5;241m=\u001b[39m \u001b[43minference_top_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_logit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[1;32m     20\u001b[0m             d[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[27], line 17\u001b[0m, in \u001b[0;36minference_top_k\u001b[0;34m(fpath, k, vname, with_logit)\u001b[0m\n\u001b[1;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# audio file is decoded on the fly\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/moon/lib/python3.9/site-packages/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py:219\u001b[0m, in \u001b[0;36mASTFeatureExtractor.__call__\u001b[0;34m(self, raw_speech, sampling_rate, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m [raw_speech]\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# extract fbank features and pad/truncate to max_length\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_fbank_features(waveform, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length) \u001b[38;5;28;01mfor\u001b[39;00m waveform \u001b[38;5;129;01min\u001b[39;00m raw_speech]\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# convert into BatchFeature\u001b[39;00m\n\u001b[1;32m    222\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m BatchFeature({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: features})\n",
      "File \u001b[0;32m~/anaconda3/envs/moon/lib/python3.9/site-packages/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py:219\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    216\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m [raw_speech]\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# extract fbank features and pad/truncate to max_length\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_fbank_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m waveform \u001b[38;5;129;01min\u001b[39;00m raw_speech]\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# convert into BatchFeature\u001b[39;00m\n\u001b[1;32m    222\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m BatchFeature({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: features})\n",
      "File \u001b[0;32m~/anaconda3/envs/moon/lib/python3.9/site-packages/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py:119\u001b[0m, in \u001b[0;36mASTFeatureExtractor._extract_fbank_features\u001b[0;34m(self, waveform, max_length)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_speech_available():\n\u001b[1;32m    118\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(waveform)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 119\u001b[0m     fbank \u001b[38;5;241m=\u001b[39m \u001b[43mta_kaldi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfbank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhanning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_mel_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_mel_bins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(waveform)\n",
      "File \u001b[0;32m~/anaconda3/envs/moon/lib/python3.9/site-packages/torchaudio/compliance/kaldi.py:633\u001b[0m, in \u001b[0;36mfbank\u001b[0;34m(waveform, blackman_coeff, channel, dither, energy_floor, frame_length, frame_shift, high_freq, htk_compat, low_freq, min_duration, num_mel_bins, preemphasis_coefficient, raw_energy, remove_dc_offset, round_to_power_of_two, sample_frequency, snip_edges, subtract_mean, use_energy, use_log_fbank, use_power, vtln_high, vtln_low, vtln_warp, window_type)\u001b[0m\n\u001b[1;32m    630\u001b[0m mel_energies \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(spectrum, mel_energies\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_log_fbank:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# avoid log of zero (which should be prevented anyway by dithering)\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m     mel_energies \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_energies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_epsilon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlog()\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# if use_energy then add it as the last column for htk_compat == true else first column\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_energy:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(int)\n",
    "\n",
    "counts = 0\n",
    "\n",
    "paths = glob(\"/home/aicontest/DF/data/audio/train/*.ogg\")\n",
    "avg = []\n",
    "tqdm_bar = tqdm(paths)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"./test_only_speech_list.txt\", \"w\") as tf:\n",
    "    for path in tqdm_bar:\n",
    "        labels, prob = inference_top_k(fpath=path, k=2, with_logit=True)\n",
    "\n",
    "        for label in labels:\n",
    "            d[label] += 1\n",
    "\n",
    "        if labels[-1] == 'Speech' and prob[-1].item() > 1.4:\n",
    "            tf.write(path + \"\\n\")\n",
    "            counts += 1\n",
    "\n",
    "        tqdm_bar.set_postfix(only_speech=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
